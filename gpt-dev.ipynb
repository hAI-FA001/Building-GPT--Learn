{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)  # ~1M characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# get unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 6, 1, 61, 53, 56, 50, 42, 2]\n",
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "# mappings, can use unordered data structure for these\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for ch, i in stoi.items()}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]  # encode a str into numbers, must use an ordererd data structuer\n",
    "decode = lambda l: ''.join([itos[i] for i in l])  # decode a list of ints\n",
    "\n",
    "print(encode(\"Hello, world!\"))\n",
    "print(decode(encode(\"Hello, world!\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ways\n",
    "- SentencePiece\n",
    "- tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.n_vocab  # ~50k, ours is 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 11, 995, 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.encode(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, world!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.decode([15496, 11, 995, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize/Encode our training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PMLS\\Desktop\\code\\py\\gpt-2-tuto\\venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:258: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1115394]), torch.int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "data.shape, data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
       "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
       "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
       "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
       "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
       "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
       "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
       "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
       "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
       "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
       "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
       "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
       "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
       "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
       "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
       "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
       "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
       "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
       "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
       "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
       "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
       "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
       "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
       "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
       "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
       "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
       "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
       "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
       "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
       "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
       "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
       "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
       "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
       "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
       "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
       "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
       "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
       "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
       "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
       "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
       "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
       "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
       "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
       "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
       "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
       "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
       "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
       "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
       "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
       "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
       "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:1000]  # is 1 big 1D tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90/10 split\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batches\n",
    "Training on whole dataset is expensive <br>\n",
    "So we divide it into chunks with some max sequence length (AKA context size, block size, batch dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]  # +1 cuz we want to predict that one, given the first `block_size` chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict on each position (from 0 to block_size) <br>\n",
    "This is not just for performance: make Transformer used to seeing contexts of length from 1 to block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given tensor([18]), predict 47\n",
      "Given tensor([18, 47]), predict 56\n",
      "Given tensor([18, 47, 56]), predict 57\n",
      "Given tensor([18, 47, 56, 57]), predict 58\n",
      "Given tensor([18, 47, 56, 57, 58]), predict 1\n",
      "Given tensor([18, 47, 56, 57, 58,  1]), predict 15\n",
      "Given tensor([18, 47, 56, 57, 58,  1, 15]), predict 47\n",
      "Given tensor([18, 47, 56, 57, 58,  1, 15, 47]), predict 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]  # we use each position, from 0 to block_size\n",
    "y = train_data[1:block_size+1]  # corresponding predictions for each position\n",
    "for t in range(block_size):\n",
    "    ctx = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"Given {ctx}, predict {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch = many sequences stacked on top of each other (as GPUs are good at parallel processing, we'll process these sequences in a batch in parallel (all at once)) <br>\n",
    "Make a batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:  torch.Size([4, 8])\n",
      "Targets:  torch.Size([4, 8])\n",
      "-------\n",
      "Given tensor([24]), predict 43\n",
      "Given tensor([24, 43]), predict 58\n",
      "Given tensor([24, 43, 58]), predict 5\n",
      "Given tensor([24, 43, 58,  5]), predict 57\n",
      "Given tensor([24, 43, 58,  5, 57]), predict 1\n",
      "Given tensor([24, 43, 58,  5, 57,  1]), predict 46\n",
      "Given tensor([24, 43, 58,  5, 57,  1, 46]), predict 43\n",
      "Given tensor([24, 43, 58,  5, 57,  1, 46, 43]), predict 39\n",
      "Given tensor([44]), predict 53\n",
      "Given tensor([44, 53]), predict 56\n",
      "Given tensor([44, 53, 56]), predict 1\n",
      "Given tensor([44, 53, 56,  1]), predict 58\n",
      "Given tensor([44, 53, 56,  1, 58]), predict 46\n",
      "Given tensor([44, 53, 56,  1, 58, 46]), predict 39\n",
      "Given tensor([44, 53, 56,  1, 58, 46, 39]), predict 58\n",
      "Given tensor([44, 53, 56,  1, 58, 46, 39, 58]), predict 1\n",
      "Given tensor([52]), predict 58\n",
      "Given tensor([52, 58]), predict 1\n",
      "Given tensor([52, 58,  1]), predict 58\n",
      "Given tensor([52, 58,  1, 58]), predict 46\n",
      "Given tensor([52, 58,  1, 58, 46]), predict 39\n",
      "Given tensor([52, 58,  1, 58, 46, 39]), predict 58\n",
      "Given tensor([52, 58,  1, 58, 46, 39, 58]), predict 1\n",
      "Given tensor([52, 58,  1, 58, 46, 39, 58,  1]), predict 46\n",
      "Given tensor([25]), predict 17\n",
      "Given tensor([25, 17]), predict 27\n",
      "Given tensor([25, 17, 27]), predict 10\n",
      "Given tensor([25, 17, 27, 10]), predict 0\n",
      "Given tensor([25, 17, 27, 10,  0]), predict 21\n",
      "Given tensor([25, 17, 27, 10,  0, 21]), predict 1\n",
      "Given tensor([25, 17, 27, 10,  0, 21,  1]), predict 54\n",
      "Given tensor([25, 17, 27, 10,  0, 21,  1, 54]), predict 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4  # number of independent sequences to process in parallel\n",
    "block_size = 8  # max len of each sequence\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    # pick batch_size number of indices, each ranging from 0 to \"len(data) - block_size\"\n",
    "    # why \"- block_size\"? to avoid out of bounds, cuz we use i+block_size below\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "\n",
    "    # stack each sequence for parallel processing (GPUs are good at that)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])  # random input sequences\n",
    "    # why \"+1\"? cuz we want to predict next token (if input starts from index i, then corresponding prediction starts from i+1)\n",
    "    # would \"+1\" in \"i+block_size+1\" give out of bounds? no, cuz this index is excluded (it'll go from i+1 to i+block_size)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])  # corresponding predictions for each position in input sequence\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "print('Inputs: ', xb.shape)\n",
    "print('Targets: ', yb.shape)\n",
    "print('-------')\n",
    "\n",
    "for b in range(batch_size): # for each sequence in this batch\n",
    "    for t in range(block_size): # for each position in current sequence\n",
    "        # \"b\" gives current sequence\n",
    "        # \"t\" gives position in current sequence\n",
    "        ctx = xb[b, :t+1]  # t+1 is exclusive, will give 0 to t\n",
    "        target = yb[b, t]\n",
    "        print(f\"Given {ctx}, predict {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
       "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
       "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
       "        [25, 17, 27, 10,  0, 21,  1, 54]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram\n",
    "The simplest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLM(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # below is our lookup table\n",
    "        # the rows will be interpreted as logits (scores) for the next token\n",
    "        # each token will read these logits for next token from this table\n",
    "        # when we train it, it'll adjust these scores\n",
    "        # so it'll be able to predict the next token given the current one\n",
    "        self.token_emb_table = nn.Embedding(vocab_size, vocab_size)  # table size CxC, where C is vocab_size\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        # dimension of idx and targets is (B, T) i.e. (batch_size, block_size)\n",
    "        # below gives tensor of shape (B, T, C)\n",
    "        # cuz each token \"t\" picks the \"t-th\" row from the table\n",
    "        # and as each row is vocab_size large, it's like each token becomes vocab_size in length\n",
    "        \n",
    "        logits = self.token_emb_table(idx)\n",
    "\n",
    "        # targets is optional cuz we don't provide it during inference/generation\n",
    "        # it's only provided/needed during training\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # F.cross_entropy() expects C to be 2nd dimension\n",
    "            # stretch out all the sequences, preserving the C dimension\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            \n",
    "            # use -ve log-likelihood as loss to know how well we're predicting\n",
    "            # when training, it'll maximize the logit associated with correct target (so that one will be picked on later predictions)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # note: the generation process for this Bigram model is inefficient\n",
    "        # cuz we're feeding the whole sequence for each forward pass\n",
    "        # but the model only looks at the last token to predict the next token\n",
    "        # why keep it inefficient?\n",
    "        # so it works without any changes in future for models that look into the past\n",
    "\n",
    "        \n",
    "        # idx is (B, T)\n",
    "        # max_new_tokens is number of tokens we want to generate\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)  # (B, T, C)\n",
    "            \n",
    "            # select the logits from last time step for each batch\n",
    "            # (B, T, C) -> (B, C)\n",
    "            logits = logits[:, -1, :]\n",
    "            # softmax turns logits into probabilities (makes them b/w 0 & 1)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # use the probs as multinomial distribution to sample 1 value (the predicted next token)\n",
    "            # samples 1 value for each batch -> (B, 1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # stack prev context and current predicted token for next iteration\n",
    "            # this makes it (B, T), (B, T+1), (B, T+2), ..., , (B, T+max_new_tokens)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 65]), tensor(4.8786, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = BigramLM(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "logits.shape, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (B, T) = (1, 1) -> 1 batch & 1 initial token\n",
    "initial = torch.zeros((1, 1), dtype=torch.long)\n",
    "\n",
    "generated_text = m.generate(initial, max_new_tokens=100)\n",
    "\n",
    "# get only first batch: (B, T+100) -> (T+100)\n",
    "# then turn it into python list instead of pytorch tensor\n",
    "generated_text = generated_text[0].tolist()\n",
    "\n",
    "decode(generated_text)  # gives random gibberish as we've not trained it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplest: SGD optimizer\n",
    "# AdamW is much more advanced and popular, works well\n",
    "# typical good learning rate: 3e-4\n",
    "# but for very small models, can get away w/ using higher learning rates\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4567582607269287\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(10_000):\n",
    "    xb, yb = get_batch(\"train\")  # get a batch w/ randomly chosen sequences\n",
    "    \n",
    "    logits, loss = m(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)  # zero out the gradient\n",
    "    loss.backward()  # calculate the loss w.r.t each parameter\n",
    "    optimizer.step()  # update the parameters\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HEThal ban, a hevese,\n",
      "FOLorere, tungsed o.\n",
      "Ande ht all com f?\n",
      "foud sas ggord d brd\n",
      "Ayay,\n",
      "Wh, whan uspre pl lllurs he ff arame,\n",
      "BR:\n",
      "Fo nnglld avexquteecet d anove! s wn Sck,\n",
      "n iroke.\n",
      "Whe INasa 'd gnt. thack; tusllthe nd azelsen s fudarave k:d; blinon ancoud h thend t y ntino brthese bufrandef t prro,\n",
      "TENERLThalld aveno ofrland Oreillongsthador ginthe l my'epr\n",
      "DI illlore\n",
      "\n",
      "Four nd ar, by'd tand e t th he:\n",
      "A py the LINTola veeden the fonokerimeds, w RLO: iongeak.\n",
      "Sor ce pose os ammachin.'s.\n",
      "the be\n",
      "G\n"
     ]
    }
   ],
   "source": [
    "initial = torch.zeros((1, 1), dtype=torch.long)\n",
    "generated_text = m.generate(initial, max_new_tokens=500)[0].tolist()\n",
    "print(decode(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
