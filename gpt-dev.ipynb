{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)  # ~1M characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# get unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 6, 1, 61, 53, 56, 50, 42, 2]\n",
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "# mappings, can use unordered data structure for these\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for ch, i in stoi.items()}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]  # encode a str into numbers, must use an ordererd data structuer\n",
    "decode = lambda l: ''.join([itos[i] for i in l])  # decode a list of ints\n",
    "\n",
    "print(encode(\"Hello, world!\"))\n",
    "print(decode(encode(\"Hello, world!\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ways\n",
    "- SentencePiece\n",
    "- tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.n_vocab  # ~50k, ours is 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 11, 995, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.encode(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, world!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.decode([15496, 11, 995, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize/Encode our training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PMLS\\Desktop\\code\\py\\gpt-2-tuto\\venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:258: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1115394]), torch.int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "data.shape, data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
       "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
       "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
       "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
       "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
       "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
       "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
       "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
       "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
       "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
       "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
       "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
       "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
       "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
       "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
       "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
       "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
       "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
       "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
       "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
       "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
       "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
       "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
       "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
       "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
       "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
       "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
       "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
       "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
       "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
       "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
       "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
       "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
       "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
       "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
       "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
       "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
       "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
       "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
       "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
       "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
       "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
       "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
       "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
       "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
       "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
       "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
       "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
       "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
       "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
       "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:1000]  # is 1 big 1D tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90/10 split\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batches\n",
    "Training on whole dataset is expensive <br>\n",
    "So we divide it into chunks with some max sequence length (AKA context size, block size, batch dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]  # +1 cuz we want to predict that one, given the first `block_size` chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict on each position (from 0 to block_size) <br>\n",
    "This is not just for performance: make Transformer used to seeing contexts of length from 1 to block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given tensor([18]), predict 47\n",
      "Given tensor([18, 47]), predict 56\n",
      "Given tensor([18, 47, 56]), predict 57\n",
      "Given tensor([18, 47, 56, 57]), predict 58\n",
      "Given tensor([18, 47, 56, 57, 58]), predict 1\n",
      "Given tensor([18, 47, 56, 57, 58,  1]), predict 15\n",
      "Given tensor([18, 47, 56, 57, 58,  1, 15]), predict 47\n",
      "Given tensor([18, 47, 56, 57, 58,  1, 15, 47]), predict 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]  # we use each position, from 0 to block_size\n",
    "y = train_data[1:block_size+1]  # corresponding predictions for each position\n",
    "for t in range(block_size):\n",
    "    ctx = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"Given {ctx}, predict {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch = many sequences stacked on top of each other (as GPUs are good at parallel processing, we'll process these sequences in a batch in parallel (all at once)) <br>\n",
    "Make a batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:  torch.Size([4, 8])\n",
      "Targets:  torch.Size([4, 8])\n",
      "-------\n",
      "Given tensor([24]), predict 43\n",
      "Given tensor([24, 43]), predict 58\n",
      "Given tensor([24, 43, 58]), predict 5\n",
      "Given tensor([24, 43, 58,  5]), predict 57\n",
      "Given tensor([24, 43, 58,  5, 57]), predict 1\n",
      "Given tensor([24, 43, 58,  5, 57,  1]), predict 46\n",
      "Given tensor([24, 43, 58,  5, 57,  1, 46]), predict 43\n",
      "Given tensor([24, 43, 58,  5, 57,  1, 46, 43]), predict 39\n",
      "Given tensor([44]), predict 53\n",
      "Given tensor([44, 53]), predict 56\n",
      "Given tensor([44, 53, 56]), predict 1\n",
      "Given tensor([44, 53, 56,  1]), predict 58\n",
      "Given tensor([44, 53, 56,  1, 58]), predict 46\n",
      "Given tensor([44, 53, 56,  1, 58, 46]), predict 39\n",
      "Given tensor([44, 53, 56,  1, 58, 46, 39]), predict 58\n",
      "Given tensor([44, 53, 56,  1, 58, 46, 39, 58]), predict 1\n",
      "Given tensor([52]), predict 58\n",
      "Given tensor([52, 58]), predict 1\n",
      "Given tensor([52, 58,  1]), predict 58\n",
      "Given tensor([52, 58,  1, 58]), predict 46\n",
      "Given tensor([52, 58,  1, 58, 46]), predict 39\n",
      "Given tensor([52, 58,  1, 58, 46, 39]), predict 58\n",
      "Given tensor([52, 58,  1, 58, 46, 39, 58]), predict 1\n",
      "Given tensor([52, 58,  1, 58, 46, 39, 58,  1]), predict 46\n",
      "Given tensor([25]), predict 17\n",
      "Given tensor([25, 17]), predict 27\n",
      "Given tensor([25, 17, 27]), predict 10\n",
      "Given tensor([25, 17, 27, 10]), predict 0\n",
      "Given tensor([25, 17, 27, 10,  0]), predict 21\n",
      "Given tensor([25, 17, 27, 10,  0, 21]), predict 1\n",
      "Given tensor([25, 17, 27, 10,  0, 21,  1]), predict 54\n",
      "Given tensor([25, 17, 27, 10,  0, 21,  1, 54]), predict 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4  # number of independent sequences to process in parallel\n",
    "block_size = 8  # max len of each sequence\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    # pick batch_size number of indices, each ranging from 0 to \"len(data) - block_size\"\n",
    "    # why \"- block_size\"? to avoid out of bounds, cuz we use i+block_size below\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "\n",
    "    # stack each sequence for parallel processing (GPUs are good at that)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])  # random input sequences\n",
    "    # why \"+1\"? cuz we want to predict next token (if input starts from index i, then corresponding prediction starts from i+1)\n",
    "    # would \"+1\" in \"i+block_size+1\" give out of bounds? no, cuz this index is excluded (it'll go from i+1 to i+block_size)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])  # corresponding predictions for each position in input sequence\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "print('Inputs: ', xb.shape)\n",
    "print('Targets: ', yb.shape)\n",
    "print('-------')\n",
    "\n",
    "for b in range(batch_size): # for each sequence in this batch\n",
    "    for t in range(block_size): # for each position in current sequence\n",
    "        # \"b\" gives current sequence\n",
    "        # \"t\" gives position in current sequence\n",
    "        ctx = xb[b, :t+1]  # t+1 is exclusive, will give 0 to t\n",
    "        target = yb[b, t]\n",
    "        print(f\"Given {ctx}, predict {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
       "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
       "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
       "        [25, 17, 27, 10,  0, 21,  1, 54]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling - Bigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLM(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # below is our lookup table\n",
    "        # the rows will be interpreted as logits (scores) for the next token\n",
    "        # each token will read these logits for next token from this table\n",
    "        # when we train it, it'll adjust these scores\n",
    "        # so it'll be able to predict the next token given the current one\n",
    "        self.token_emb_table = nn.Embedding(vocab_size, vocab_size)  # table size CxC, where C is vocab_size\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        # dimension of idx and targets is (B, T) i.e. (batch_size, block_size)\n",
    "        # below gives tensor of shape (B, T, C)\n",
    "        # cuz each token \"t\" picks the \"t-th\" row from the table\n",
    "        # and as each row is vocab_size large, it's like each token becomes vocab_size in length\n",
    "        \n",
    "        logits = self.token_emb_table(idx)\n",
    "\n",
    "        # targets is optional cuz we don't provide it during inference/generation\n",
    "        # it's only provided/needed during training\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # F.cross_entropy() expects C to be 2nd dimension\n",
    "            # stretch out all the sequences, preserving the C dimension\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            \n",
    "            # use -ve log-likelihood as loss to know how well we're predicting\n",
    "            # when training, it'll maximize the logit associated with correct target (so that one will be picked on later predictions)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # note: the generation process for this Bigram model is inefficient\n",
    "        # cuz we're feeding the whole sequence for each forward pass\n",
    "        # but the model only looks at the last token to predict the next token\n",
    "        # why keep it inefficient?\n",
    "        # so it works without any changes in future for models that look into the past\n",
    "\n",
    "        \n",
    "        # idx is (B, T)\n",
    "        # max_new_tokens is number of tokens we want to generate\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)  # (B, T, C)\n",
    "            \n",
    "            # select the logits from last time step for each batch\n",
    "            # (B, T, C) -> (B, C)\n",
    "            logits = logits[:, -1, :]\n",
    "            # softmax turns logits into probabilities (makes them b/w 0 & 1)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # use the probs as multinomial distribution to sample 1 value (the predicted next token)\n",
    "            # samples 1 value for each batch -> (B, 1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # stack prev context and current predicted token for next iteration\n",
    "            # this makes it (B, T), (B, T+1), (B, T+2), ..., , (B, T+max_new_tokens)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 65]), tensor(4.8786, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = BigramLM(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "logits.shape, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (B, T) = (1, 1) -> 1 batch & 1 initial token\n",
    "initial = torch.zeros((1, 1), dtype=torch.long)\n",
    "\n",
    "generated_text = m.generate(initial, max_new_tokens=100)\n",
    "\n",
    "# get only first batch: (B, T+100) -> (T+100)\n",
    "# then turn it into python list instead of pytorch tensor\n",
    "generated_text = generated_text[0].tolist()\n",
    "\n",
    "decode(generated_text)  # gives random gibberish as we've not trained it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training - Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplest: SGD optimizer\n",
    "# AdamW is much more advanced and popular, works well\n",
    "# typical good learning rate: 3e-4\n",
    "# but for very small models, can get away w/ using higher learning rates\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5727508068084717\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(10_000):\n",
    "    xb, yb = get_batch(\"train\")  # get a batch w/ randomly chosen sequences\n",
    "    \n",
    "    logits, loss = m(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)  # zero out the gradient\n",
    "    loss.backward()  # calculate the loss w.r.t each parameter\n",
    "    optimizer.step()  # update the parameters\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iyoteng h hasbe pave pirance\n",
      "Rie hicomyonthar's\n",
      "Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\n",
      "KIN d pe wither vouprrouthercc.\n",
      "hathe; d!\n",
      "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\n",
      "h hay.JUCle n prids, r loncave w hollular s O:\n",
      "HIs; ht anjx?\n",
      "\n",
      "DUThinqunt.\n",
      "\n",
      "LaZAnde.\n",
      "athave l.\n",
      "KEONH:\n",
      "ARThanco be y,-hedarwnoddy scace, tridesar, wnl'shenous s ls, theresseys\n",
      "PlorseelapinghiybHen yof GLUCEN t l-t E:\n",
      "I hisgothers je are!-e!\n",
      "QLYotouciullle'z\n"
     ]
    }
   ],
   "source": [
    "initial = torch.zeros((1, 1), dtype=torch.long)\n",
    "generated_text = m.generate(initial, max_new_tokens=500)[0].tolist()\n",
    "print(decode(generated_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to make tokens communicate with each other <br>\n",
    "but in a special way (tokens should not communicate with future tokens) <br>\n",
    "<br>\n",
    "Simplest \"Communication\": Average the previous elements/time steps/tokens <br>\n",
    "e.g. For 5th token, average the channels of tokens from 1 to 5 and use it as 5th token's channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1808, -0.0700])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x[b, t] will be mean of x[b, i] where i=1 to t\n",
    "# bow = bag of words = term used when simply averaging things\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        # for current sequence in batch, get tokens from index 0 to t (first t tokens)\n",
    "        xprev = x[b, :t+1]  # (t, C)\n",
    "        # average the 0th dim (first dim)\n",
    "        # (t, C) -> (1, C)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)\n",
    "xbow[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical trick in Self-Attention\n",
    "Above implementation is inefficient (uses for-loop) <br>\n",
    "Efficient way would use matrix multiplication (cuz GPUs are good at that), AKA \"vectorization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[8., 6.],\n",
      "        [5., 2.],\n",
      "        [4., 4.]])\n",
      "tensor([[17., 12.],\n",
      "        [17., 12.],\n",
      "        [17., 12.]])\n"
     ]
    }
   ],
   "source": [
    "# matrix multiplication will help in efficient implementation\n",
    "# matrix c = a @ b here will be sum along columns of b\n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[7., 4.],\n",
      "        [5., 0.],\n",
      "        [5., 3.]])\n",
      "tensor([[ 7.,  4.],\n",
      "        [12.,  4.],\n",
      "        [17.,  7.]])\n"
     ]
    }
   ],
   "source": [
    "# if we use a lower triangular matrix of all 1s,\n",
    "# then each row of c will be sum along columns of b till a certain element\n",
    "# e.g. 1st row of c = sum from 1st element to 1st element (so it's just the first element)\n",
    "# e.g. 2nd row of c = sum from 1st element to 2nd element\n",
    "# e.g. 3rd row of c = sum from 1st element to 3rd element\n",
    "# => can see \"ith\" row of c uses first \"i\" elements of b (along column)\n",
    "# it's similar to what we want (average along first \"t\" elements)\n",
    "\n",
    "a = torch.tril(torch.ones(3, 3))  # tril -> make lower triangular matrix from given matrix\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[8., 9.],\n",
      "        [2., 7.],\n",
      "        [3., 9.]])\n",
      "tensor([[8.0000, 9.0000],\n",
      "        [5.0000, 8.0000],\n",
      "        [4.3333, 8.3333]])\n"
     ]
    }
   ],
   "source": [
    "# finally, for average, we need to normalize \"a\" (divide each row by sum of elements in that row)\n",
    "# why/what's \"normalize\"?\n",
    "#     e.g. (1+2)/2 is average, we can write it like 1/2 * (1+2) = 0.5 * (1+2)\n",
    "#     in prev cell, each row of \"c\" is a sum (doesn't multiply or divide by anything)\n",
    "#     to get the multiplication number for each row, we \"normalize\" the rows of \"a\" (i.e. divide by sum of elements along that row)\n",
    "#         e.g. row of a = [1 1 0], sum is 1+1+0 = 2, so divide by 2 -> [0.5 0.5 0]\n",
    "#         now, for this row, a @ b will give: 0.5 * (the sum), i.e. the average\n",
    "\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficient (or \"vectorized\") Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "weights = torch.tril(torch.ones(T, T))\n",
    "weights = weights / weights.sum(1, keepdim=True)\n",
    "print(weights)\n",
    "\n",
    "# for 1 sequence: (T, T) @ (T, C) -> (T, C)\n",
    "# for whole batch: (T, T) @ (B, T, C) -> (B, T, T) @ (B, T, C) = (B, T, C) \n",
    "xbow_vec = weights @ x\n",
    "\n",
    "print(torch.allclose(xbow, xbow_vec, 1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another version <br>\n",
    "This one will be used: <br>\n",
    " - Weights are initialized from zero (like other weights) and we can think of their initial value as \"affinity\"/interaction strength <br>\n",
    " - These affinities will be trained <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "print(tril)\n",
    "weights = torch.zeros((T, T))\n",
    "# where tril contains 0, put -inf there\n",
    "print(weights)\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))  # tokens from past won't communicate with future\n",
    "print(weights)\n",
    "# softmax(z_ij) = (e^z_ij)/sum_i(e^z_ij)\n",
    "# z_ij represents an element in the matrix, it can either be 0 or -inf for this matrix\n",
    "# so e^z_ij will either be e^0 = 1 or e^-inf = 0\n",
    "# when z_ij is -inf, softmax = 0\n",
    "# when z_ij is 0, softmax will put normalized value\n",
    "#     sum_i(e^z_ij) sums along the \"ith\" row\n",
    "#     e.g. for 2nd row which is [0 0 -inf -inf ... -inf]: (e^0 + e^0 + e^-inf + e^-inf + ... + -inf) = (1 + 1 + 0 + 0 + ... + 0) = 2\n",
    "#     so each element e^z_ij will be divided by sum of elements e^z_ij (it will be normalized)\n",
    "print(torch.exp(weights))  # to check e^z_ij\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "print(weights)\n",
    "\n",
    "xbow3 = weights @ x\n",
    "\n",
    "torch.allclose(xbow, xbow3, 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention\n",
    "Right now, \"weights\" contains uniform values in each row <br>\n",
    "But we want them to be different\n",
    " - idea: tokens will find some past tokens more \"interesting\" or helpful for the prediction task\n",
    "\n",
    "Self-Attention allows info to flow from past in data-dependent way <br>\n",
    " - Every token will emit 2 vectors: Key and Query\n",
    "   - Query: \"What am I looking for?\"\n",
    "   - Key: \"What do I contain?\"\n",
    " - To get affinities b/w tokens, we perform dot-product b/w Keys and Queries\n",
    "   - e.g. token 5's Query will be dot-product-ed with the other tokens' Keys\n",
    "   - If a Key and Query interact to a very high amount, then token 5 will learn more about the token associated with that Key\n",
    " - For aggregation, we don't use average:\n",
    "   - we have another vector called Value for each token\n",
    "     - Value: \"here's what I have\", \"here's my identity\", \"here's what I'll communicate\"\n",
    "     - we don't aggregate the raw input tokens, but we aggregate their Value vectors\n",
    "   - we send the dot-product of Query and Key to Softmax to normalize it\n",
    "   - the output from Softmax is then multiplied with Value\n",
    " - so, Attention(Q, K, V) = softmax(Q @ K) @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q @ K:\n",
      " tensor([[-1.7629, -1.3011,  0.5652,  2.1616, -1.0674,  1.9632,  1.0765, -0.4530],\n",
      "        [-3.3334, -1.6556,  0.1040,  3.3782, -2.1825,  1.0415, -0.0557,  0.2927],\n",
      "        [-1.0226, -1.2606,  0.0762, -0.3813, -0.9843, -1.4303,  0.0749, -0.9547],\n",
      "        [ 0.7836, -0.8014, -0.3368, -0.8496, -0.5602, -1.1701, -1.2927, -1.0260],\n",
      "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,  0.8638,  0.3719,  0.9258],\n",
      "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,  1.4187,  1.2196],\n",
      "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,  0.8048],\n",
      "        [-1.8044, -0.4126, -0.8306,  0.5898, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "Masked:\n",
      " tensor([[-1.7629,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-3.3334, -1.6556,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-1.0226, -1.2606,  0.0762,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.7836, -0.8014, -0.3368, -0.8496,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,    -inf,    -inf,    -inf],\n",
      "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,    -inf,    -inf],\n",
      "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,    -inf],\n",
      "        [-1.8044, -0.4126, -0.8306,  0.5898, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "Softmax(Q @ K):\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
      "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
      "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "Final attention output:\n",
      " tensor([[-0.1571,  0.8801,  0.1615, -0.7824, -0.1429,  0.7468,  0.1007, -0.5239,\n",
      "         -0.8873,  0.1907,  0.1762, -0.5943, -0.4812, -0.4860,  0.2862,  0.5710],\n",
      "        [ 0.6764, -0.5477, -0.2478,  0.3143, -0.1280, -0.2952, -0.4296, -0.1089,\n",
      "         -0.0493,  0.7268,  0.7130, -0.1164,  0.3266,  0.3431, -0.0710,  1.2716],\n",
      "        [ 0.4823, -0.1069, -0.4055,  0.1770,  0.1581, -0.1697,  0.0162,  0.0215,\n",
      "         -0.2490, -0.3773,  0.2787,  0.1629, -0.2895, -0.0676, -0.1416,  1.2194],\n",
      "        [ 0.1971,  0.2856, -0.1303, -0.2655,  0.0668,  0.1954,  0.0281, -0.2451,\n",
      "         -0.4647,  0.0693,  0.1528, -0.2032, -0.2479, -0.1621,  0.1947,  0.7678],\n",
      "        [ 0.2510,  0.7346,  0.5939,  0.2516,  0.2606,  0.7582,  0.5595,  0.3539,\n",
      "         -0.5934, -1.0807, -0.3111, -0.2781, -0.9054,  0.1318, -0.1382,  0.6371],\n",
      "        [ 0.3428,  0.4960,  0.4725,  0.3028,  0.1844,  0.5814,  0.3824,  0.2952,\n",
      "         -0.4897, -0.7705, -0.1172, -0.2541, -0.6892,  0.1979, -0.1513,  0.7666],\n",
      "        [ 0.1866, -0.0964, -0.1430,  0.3059,  0.0834, -0.0069, -0.2047, -0.1535,\n",
      "         -0.0762,  0.3269,  0.3090,  0.0766,  0.0992,  0.1656,  0.1975,  0.7625],\n",
      "        [ 0.1301, -0.0328, -0.4965,  0.2865,  0.2704, -0.2636, -0.0738,  0.3786,\n",
      "          0.0746,  0.0338,  0.0147,  0.3194,  0.2993, -0.1653, -0.0386,  0.3375]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "\n",
    "# addition to prev code\n",
    "head_size = 16  # new hyperparam\n",
    "\n",
    "# these take in a vector of size C and give a vector of size head_size\n",
    "# bias=False means instead of \"W @ Input + b\" it's just \"W @ Input\" (no value is added)\n",
    "#    W will be of size (C, head_size) cuz (1, C) @ (C, head_size) = (1, head_size) which is the size we specified\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# get the Key and Query vector associated with all tokens of all sequences in batch \"x\"\n",
    "# these are (B, T, head_size) cuz we feed (B, T, C) rather than (1, C)\n",
    "#    (B, T, C) @ (C, head_size) -> (B, T, head_size), not (1, head_size)\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "\n",
    "# now the Queries and Keys interact\n",
    "# (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "# the (T, T) dim means \"affinity of all tokens with all other tokens\" in that sequence\n",
    "weights = q @ k.transpose(-2, -1)  # transpose the last 2 dims so they have compatible dims for matrix multiplication\n",
    "print(\"Q @ K:\\n\", weights[0])\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# weights = torch.zeros((T, T))  # we initialized it with affinities above\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))  # tokens from past won't communicate with future\n",
    "print(\"\\nMasked:\\n\", weights[0])\n",
    "weights = F.softmax(weights, dim=-1)  # normalize\n",
    "print(\"\\nSoftmax(Q @ K):\\n\", weights[0])\n",
    "\n",
    "# out = weights @ x  # not the aggregation we use\n",
    "v = value(x)\n",
    "out = weights @ v  # now output is (B, T, head_size)\n",
    "print(\"\\nFinal attention output:\\n\", out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 8]), torch.Size([4, 8, 16]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
